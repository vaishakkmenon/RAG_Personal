# ============================================================================
# Production Docker Compose - Optimized for Groq-Only Deployment with Caddy
# ============================================================================
# Changes from development:
#   ✓ Using Groq exclusively for LLM inference
#   ✓ Removed test service (testing done in CI/CD)
#   ✓ Removed hot-reload volume mounts (code baked into image)
#   ✓ Removed eval/analytics directories (production doesn't need these)
#   ✓ Enabled resource limits
#   ✓ Added Caddy reverse proxy with automatic HTTPS
#   ✓ API service not exposed to host (only accessible via Caddy)
# ============================================================================

name: personal-rag-system-prod

services:
  # ==========================================================================
  # Caddy Reverse Proxy - Automatic HTTPS with Let's Encrypt
  # ==========================================================================
  caddy:
    image: caddy:2-alpine
    container_name: rag_caddy
    hostname: caddy

    # Expose HTTP and HTTPS ports to the host
    ports:
      - "80:80"
      - "443:443"
      - "443:443/udp"  # HTTP/3 support

    # Reverse proxy configuration
    # Caddy will automatically obtain and renew SSL certificates from Let's Encrypt
    # Reverse proxy configuration using Caddyfile
    # command: caddy reverse-proxy --from api.vaishakmenon.com --to api:8000
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy_data:/data
      - caddy_config:/config
      - ./logs/caddy:/var/log/caddy  # Persist logs

    depends_on:
      api:
        condition: service_healthy

    restart: always

    # Security hardening
    security_opt:
      - no-new-privileges:true

    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M

    # Logging
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=caddy,env=production"

    networks:
      - rag_network
  # ==========================================================================
  # FastAPI API Service - Main application (Groq-only)
  # ==========================================================================
  api:
    build:
      context: .
      dockerfile: Dockerfile.prod
      target: runtime
      args:
        - BUILDKIT_INLINE_CACHE=1
    image: personal-rag-system:prod
    container_name: rag_api_prod
    hostname: api

    # NO ports exposed - only accessible via Caddy reverse proxy
    # ports:
    #   - "${API_PORT:-8000}:8000"

    environment:
      # ============ Application Settings ============
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - PYTHONUNBUFFERED=1

      # ============ LLM Provider (Groq ONLY) ============
      - LLM_PROVIDER=groq
      - LLM_GROQ_API_KEY=${LLM_GROQ_API_KEY}  # REQUIRED - no default
      - LLM_GROQ_MODEL=${LLM_GROQ_MODEL:-llama-3.1-8b-instant}
      - LLM_TEMPERATURE=${LLM_TEMPERATURE:-0.1}
      - LLM_MAX_TOKENS=${LLM_MAX_TOKENS:-1000}

      # ============ Retrieval Settings ============
      - TOP_K=${TOP_K:-5}
      - MAX_DISTANCE=${MAX_DISTANCE:-0.8}

      # ============ Session Management (Redis) ============
      - SESSION_STORAGE_BACKEND=redis
      - SESSION_REDIS_URL=redis://redis:6379/0
      - SESSION_MAX_TOTAL_SESSIONS=${SESSION_MAX_TOTAL_SESSIONS:-1000}
      - SESSION_MAX_SESSIONS_PER_IP=${SESSION_MAX_SESSIONS_PER_IP:-5}
      - SESSION_QUERIES_PER_HOUR=${SESSION_QUERIES_PER_HOUR:-10}
      - SESSION_TTL_SECONDS=${SESSION_TTL_SECONDS:-10800}        # 3 hours
      - SESSION_MAX_HISTORY_TOKENS=${SESSION_MAX_HISTORY_TOKENS:-150}
      - SESSION_MAX_HISTORY_TURNS=${SESSION_MAX_HISTORY_TURNS:-5}

      # ============ Admin Authentication ============
      - ADMIN_USER=${ADMIN_USER:-admin}
      - ADMIN_PASSWORD=${ADMIN_PASSWORD}

      # ============ Query Rewriting ============
      - QUERY_REWRITER_ENABLED=${QUERY_REWRITER_ENABLED:-false}

      # ============ HuggingFace Cache ============
      - HF_HOME=/tmp/huggingface
      - HUGGINGFACE_HUB_CACHE=/tmp/huggingface

      # ============ Uvicorn Server Config ============
      - UVICORN_WORKERS=${UVICORN_WORKERS:-2}
      - UVICORN_LOG_LEVEL=${LOG_LEVEL:-info}

    env_file:
      - .env  # Production environment file

    volumes:
      # Production: NO source code mounts (baked into image)
      # Only persistent data directories
      - ./data/chroma:/workspace/data/chroma
      - ./data/docs:/workspace/data/docs:ro
      - ./data/mds:/workspace/data/mds:ro
      - ./config:/workspace/config:ro

    depends_on:
      redis:
        condition: service_healthy

    # Security hardening
    security_opt:
      - no-new-privileges:true

    read_only: true

    cap_drop:
      - ALL

    tmpfs:
      - /tmp:noexec,nosuid,nodev,size=512m

    user: "65532:65532"

    # Production uses baked-in entrypoint/command from Dockerfile
    # Override only if needed

    healthcheck:
      test:
        - CMD
        - /opt/venv/bin/python
        - -c
        - "import socket; s=socket.socket(); s.settimeout(5); s.connect(('127.0.0.1', 8000)); s.close()"
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

    restart: always

    # Graceful shutdown: Allow time for in-flight requests to complete
    # Docker sends SIGTERM, waits this period, then sends SIGKILL
    stop_grace_period: 30s

    # Resource limits (ENABLED for production)
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"  # Keep more logs in production
        labels: "service=api,env=production"

    networks:
      - rag_network

  # ==========================================================================
  # PostgreSQL Service - Core Database (Production)
  # ==========================================================================
  postgres:
    image: postgres:15-alpine
    container_name: rag_postgres_prod
    hostname: postgres

    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}

    # No ports exposed to host for security
    # Access only via internal Docker network (api, exporters)

    volumes:
      - postgres_data_prod:/var/lib/postgresql/data

    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

    restart: always

    security_opt:
      - no-new-privileges:true

    # Resource limits (ENABLED for production)
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=postgres,env=production"

    networks:
      - rag_network

  redis:
    image: redis:7-alpine
    container_name: rag_redis_prod
    hostname: redis

    command:
      - redis-server
      - --appendonly yes
      - --appendfsync everysec
      - --maxmemory 512mb              # Increased for production
      - --maxmemory-policy allkeys-lru
      - --save 60 1
      - --loglevel notice
      - --tcp-backlog 511
      - --timeout 300
      - --tcp-keepalive 300
      # Production optimizations
      - --maxclients 10000             # Higher connection limit
      - --protected-mode yes           # Security

    # Bind to localhost only - Redis should not be accessible from outside the host
    ports:
      - "127.0.0.1:${REDIS_PORT:-6379}:6379"

    volumes:
      - redis_data_prod:/data

    healthcheck:
      test:
        - CMD
        - redis-cli
        - --raw
        - incr
        - ping
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

    security_opt:
      - no-new-privileges:true

    restart: always

    # Resource limits (ENABLED for production)
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 768M
        reservations:
          cpus: '0.5'
          memory: 512M

    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=redis,env=production"

    networks:
      - rag_network

  # ==========================================================================
  # Prometheus Service - Metrics collection and alerting (Production)
  # ==========================================================================
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: rag_prometheus_prod
    hostname: prometheus

    # Bind to localhost only - access Prometheus via SSH tunnel if needed
    ports:
      - "127.0.0.1:9090:9090"

    volumes:
      - ./monitoring/prometheus/prometheus.prod.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro
      - prometheus_data_prod:/prometheus

    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'

    depends_on:
      api:
        condition: service_healthy

    restart: always

    # Security hardening
    security_opt:
      - no-new-privileges:true

    # Prometheus runs as nobody (65534) by default
    user: "65534:65534"

    # Resource limits (ENABLED for production)
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M

    # Health check
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=prometheus,env=production"

    networks:
      - rag_network

  # ==========================================================================
  # Discord Webhook Adapter - Converts Alertmanager format to Discord format
  # ==========================================================================
  alertmanager-discord:
    image: benjojo/alertmanager-discord:latest
    container_name: rag_alertmanager_discord_prod
    environment:
      - DISCORD_WEBHOOK=${DISCORD_WEBHOOK_URL}
    restart: always
    networks:
      - rag_network
    security_opt:
      - no-new-privileges:true

  # ==========================================================================
  # Alertmanager Service - Alert notifications (Production)
  # ==========================================================================
  alertmanager:
    image: prom/alertmanager:v0.26.0
    container_name: rag_alertmanager_prod
    hostname: alertmanager
    ports:
      - "127.0.0.1:9093:9093"
    volumes:
      - ./monitoring/prometheus/alertmanager.yml.template:/etc/alertmanager/alertmanager.yml.template:ro
      - alertmanager_data_prod:/alertmanager
    # Use sed to substitute environment variables into the config template
    # (Alertmanager doesn't support env var substitution natively)
    entrypoint:
      - /bin/sh
      - -c
      - |
        sed -e "s|\$${SMTP_FROM}|$$SMTP_FROM|g" \
            -e "s|\$${SMTP_USERNAME}|$$SMTP_USERNAME|g" \
            -e "s|\$${SMTP_PASSWORD}|$$SMTP_PASSWORD|g" \
            -e "s|\$${ALERT_EMAIL}|$$ALERT_EMAIL|g" \
            -e "s|\$${DISCORD_WEBHOOK_URL}|$$DISCORD_WEBHOOK_URL|g" \
            /etc/alertmanager/alertmanager.yml.template > /tmp/alertmanager.yml && \
        exec /bin/alertmanager --config.file=/tmp/alertmanager.yml --storage.path=/alertmanager
    environment:
      - SMTP_FROM=${SMTP_FROM}
      - SMTP_USERNAME=${SMTP_USERNAME}
      - SMTP_PASSWORD=${SMTP_PASSWORD}
      - ALERT_EMAIL=${ALERT_EMAIL}
      - DISCORD_WEBHOOK_URL=${DISCORD_WEBHOOK_URL}
    restart: always
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 128M
        reservations:
          cpus: '0.1'
          memory: 64M
    networks:
      - rag_network

  # ==========================================================================
  # Grafana Service - Metrics visualization and dashboards (Production)
  # ==========================================================================
  grafana:
    image: grafana/grafana:10.2.2
    container_name: rag_grafana_prod
    hostname: grafana

    # Bind to localhost only - access Grafana via SSH tunnel or Caddy subdomain
    ports:
      - "127.0.0.1:3000:3000"

    environment:
      # Admin credentials (configurable via .env.prod)
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}  # REQUIRED in production

      # Server configuration
      - GF_SERVER_ROOT_URL=${GRAFANA_ROOT_URL:-https://grafana.vaishakmenon.com}
      - GF_SERVER_SERVE_FROM_SUB_PATH=false

      # Anonymous access disabled
      - GF_AUTH_ANONYMOUS_ENABLED=false

      # Provisioning
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning

      # Default home dashboard
      - GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH=/etc/grafana/dashboards/01-system-overview.json

    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/etc/grafana/dashboards:ro
      - grafana_data_prod:/var/lib/grafana

    depends_on:
      prometheus:
        condition: service_healthy

    restart: always

    # Security hardening
    security_opt:
      - no-new-privileges:true

    # Grafana runs as grafana user (472) by default
    user: "472:472"

    # Resource limits (ENABLED for production)
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

    # Health check
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=grafana,env=production"

    networks:
      - rag_network

  # ==========================================================================
  # Redis Exporter - Prometheus metrics for Redis (Production)
  # ==========================================================================
  redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: rag_redis_exporter_prod
    hostname: redis-exporter
    environment:
      - REDIS_ADDR=redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    depends_on:
      redis:
        condition: service_healthy
    restart: always
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '0.1'
          memory: 64M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - rag_network

  # ==========================================================================
  # PostgreSQL Exporter - Prometheus metrics for PostgreSQL (Production)
  # ==========================================================================
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: rag_postgres_exporter_prod
    hostname: postgres-exporter
    environment:
      - DATA_SOURCE_NAME=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}?sslmode=disable
    depends_on:
      postgres:
        condition: service_healthy
    restart: always
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '0.1'
          memory: 64M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - rag_network

  # ==========================================================================
  # Backup Service - Automated Nightly Backups (Production)
  # ==========================================================================
  backup:
    image: postgres:15-alpine
    container_name: rag_backup_prod
    hostname: backup
    restart: unless-stopped

    # Run the backup script every 24 hours
    command: >
      sh -c "while true; do
        echo 'Running daily backup...';
        /scripts/backup.sh;
        echo 'Sleeping for 24 hours...';
        sleep 86400;
      done"

    volumes:
      - ./backups:/backups
      - ./scripts:/scripts:ro
      - redis_data_prod:/var/lib/redis:ro   # Read-only access to Redis data
      - ./data/chroma:/chroma_data:ro       # Read-only access to Chroma data

    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - BACKUP_DIR=/backups

    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

    networks:
      - rag_network

# ============================================================================
# Named Volumes - Persistent data storage
# ============================================================================
volumes:
  redis_data_prod:
    driver: local
    name: rag_redis_data_prod

  # PostgreSQL data persistence
  postgres_data_prod:
    driver: local
    name: rag_postgres_data_prod

  # Caddy data for SSL certificates
  caddy_data:
    driver: local
    name: rag_caddy_data

  # Caddy config
  caddy_config:
    driver: local
    name: rag_caddy_config

  # Prometheus time-series data (production)
  prometheus_data_prod:
    driver: local
    name: rag_prometheus_data_prod

  # Grafana dashboards and settings (production)
  grafana_data_prod:
    driver: local
    name: rag_grafana_data_prod

  # Alertmanager data (production)
  alertmanager_data_prod:
    driver: local
    name: rag_alertmanager_data_prod

# ============================================================================
# Networks - Internal communication
# ============================================================================
networks:
  rag_network:
    driver: bridge
    name: rag_network_prod

# ============================================================================
# Production Usage with Caddy
# ============================================================================
# Prerequisites:
#   - DNS A record: api.vaishakmenon.com -> VPS IP address
#   - Ports 80 and 443 open on VPS firewall
#
# 1. Create .env.prod with production secrets:
#    LLM_GROQ_API_KEY=your-key-here
#
# 2. Build production image:
#    docker-compose -f docker-compose.prod.yml build
#
# 3. Start services:
#    docker-compose -f docker-compose.prod.yml up -d
#
# 4. Caddy will automatically:
#    - Obtain SSL certificate from Let's Encrypt
#    - Set up HTTPS on port 443
#    - Redirect HTTP (port 80) to HTTPS
#    - Auto-renew certificates
#
# 5. Access API:
#    https://api.vaishakmenon.com/health
#    https://api.vaishakmenon.com/docs
#
# 6. View logs:
#    docker-compose -f docker-compose.prod.yml logs -f
#    docker-compose -f docker-compose.prod.yml logs -f caddy
#
# 7. Check health:
#    docker-compose -f docker-compose.prod.yml ps
#
# 8. Stop services:
#    docker-compose -f docker-compose.prod.yml down
#
# Resource usage (estimated):
#   - API: 2-4GB RAM, 1-2 CPU cores
#   - Redis: 512-768MB RAM, 0.5-1 CPU
#   - Caddy: 128-256MB RAM, 0.25-0.5 CPU
#   - Total: ~3-5GB RAM, 2-3 CPU cores
#
# Image size (estimated):
#   - Development: ~3GB
#   - Production: ~2GB (no dev tools)
#
# Important:
#   - API service is NOT exposed on port 8000 externally
#   - Only Caddy is exposed (ports 80, 443)
#   - All traffic goes through Caddy reverse proxy with HTTPS
# ============================================================================
