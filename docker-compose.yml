# ============================================================================
# Personal RAG System - Docker Compose Configuration
# ============================================================================
# Services:
#   - api:    FastAPI backend with ChromaDB
#   - redis:  Session storage and caching
#   - ollama: Local LLM inference (fallback)
#   - test:   Testing environment
# ============================================================================

name: personal-rag-system

services:
  # ==========================================================================
  # FastAPI API Service - Main application
  # ==========================================================================
  api:
    build:
      context: .
      target: runtime
      args:
        - BUILDKIT_INLINE_CACHE=1
    image: personal-rag-system:latest
    container_name: rag_api
    hostname: api

    # Port mapping: host:container
    ports:
      - "${API_PORT:-8000}:8000"

    # Environment variables (with sensible defaults)
    environment:
      # ============ Application Settings ============
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - PYTHONUNBUFFERED=1
      - WATCHFILES_FORCE_POLLING=1

      # ============ LLM Provider Configuration ============
      # Primary: Groq (cloud API)
      - LLM_PROVIDER=${LLM_PROVIDER:-groq}
      - LLM_GROQ_API_KEY=${LLM_GROQ_API_KEY:-}
      - LLM_GROQ_MODEL=${LLM_GROQ_MODEL:-llama-3.1-70b-versatile}

      # Fallback: Ollama (local inference)
      - LLM_FALLBACK_PROVIDER=ollama
      - LLM_OLLAMA_HOST=http://ollama:11434
      - LLM_OLLAMA_MODEL=${LLM_OLLAMA_MODEL:-llama3.1:8b}

      # LLM generation parameters
      - LLM_TEMPERATURE=${LLM_TEMPERATURE:-0.1}
      - LLM_MAX_TOKENS=${LLM_MAX_TOKENS:-1000}

      # ============ Session Management (Redis) ============
      - SESSION_STORAGE_BACKEND=${SESSION_STORAGE_BACKEND:-redis}
      - SESSION_REDIS_URL=redis://:${REDIS_PASSWORD:-devpassword123}@redis:6379/0
      - SESSION_MAX_TOTAL_SESSIONS=${SESSION_MAX_TOTAL_SESSIONS:-1000}
      - SESSION_MAX_SESSIONS_PER_IP=${SESSION_MAX_SESSIONS_PER_IP:-100}  # Increased for testing
      - SESSION_QUERIES_PER_HOUR=${SESSION_QUERIES_PER_HOUR:-1000}  # Increased for testing
      - SESSION_TTL_SECONDS=${SESSION_TTL_SECONDS:-21600}
      - SESSION_MAX_HISTORY_TOKENS=${SESSION_MAX_HISTORY_TOKENS:-250}
      - SESSION_MAX_HISTORY_TURNS=${SESSION_MAX_HISTORY_TURNS:-5}

      # ============ Query Rewriting (Disabled - causes regressions) ============
      - QUERY_REWRITER_ENABLED=${QUERY_REWRITER_ENABLED:-false}

      # ============ HuggingFace Cache (redirect to writable /tmp) ============
      - HF_HOME=/tmp/huggingface

      - HUGGINGFACE_HUB_CACHE=/tmp/huggingface

      # ============ Uvicorn Server Config ============
      - UVICORN_WORKERS=${UVICORN_WORKERS:-2}
      - UVICORN_LOG_LEVEL=${LOG_LEVEL:-info}

    # Load additional env vars from .env file
    env_file:
      - .env

    # Volume mounts
    volumes:
      # Application code (for development hot-reload)
      - ./app:/workspace/app:ro

      # Scripts for evaluation and utilities
      - ./scripts:/workspace/scripts:ro

      # Environment file (read-only)
      - ./.env:/workspace/.env:ro

      # Configuration files
      - ./config:/workspace/config:ro

      # Data directories
      - ./data/chroma:/workspace/data/chroma
      - ./data/docs:/workspace/data/docs
      - ./data/mds:/workspace/data/mds:ro
      - ./data/eval:/workspace/data/eval
      - ./data/analytics:/workspace/data/analytics

    # Service dependencies (with health checks)
    depends_on:
      redis:
        condition: service_healthy
      ollama:
        condition: service_started

    # Security hardening
    security_opt:
      - no-new-privileges:true

    read_only: true

    # Drop all capabilities (principle of least privilege)
    cap_drop:
      - ALL

    # Writable /tmp for runtime files and model cache
    tmpfs:
      - /tmp:noexec,nosuid,nodev,size=512m

    # Run as non-root user (nonroot:nonroot = 65532:65532)
    user: "65532:65532"

    # Use entrypoint script for initialization, then start uvicorn
    entrypoint: ["/bin/bash", "/workspace/scripts/docker-entrypoint.sh"]
    command:
      - /opt/venv/bin/python
      - -m
      - uvicorn
      - app.main:app
      - --host
      - "0.0.0.0"
      - --port
      - "8000"
      - --workers
      - "${UVICORN_WORKERS:-2}"
      - --log-level
      - "${LOG_LEVEL:-info}"

    # Health check (critical for depends_on conditions)
    healthcheck:
      test:
        - CMD
        - /opt/venv/bin/python
        - -c
        - "import socket; s=socket.socket(); s.settimeout(5); s.connect(('127.0.0.1', 8000)); s.close(); print('âœ… API healthy')"
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

    # Restart policy
    restart: unless-stopped

    # Resource limits (optional, uncomment for production)
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '2.0'
    #       memory: 4G
    #     reservations:
    #       cpus: '1.0'
    #       memory: 2G

    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=api"

  # ==========================================================================
  # Redis Service - Session storage and caching
  # ==========================================================================
  redis:
    image: redis:7-alpine
    container_name: rag_redis
    hostname: redis

    # Redis server configuration
    command:
      - redis-server
      - --appendonly yes                    # Enable AOF persistence
      - --requirepass ${REDIS_PASSWORD:-devpassword123}  # Require encryption
      - --appendfsync everysec              # Fsync every second (balanced)
      - --maxmemory 256mb                   # Memory limit
      - --maxmemory-policy allkeys-lru      # Eviction policy
      - --save 60 1                         # RDB snapshot: every 60s if 1 key changed
      - --loglevel notice                   # Logging level
      - --tcp-backlog 511                   # Connection queue size
      - --timeout 300                       # Close idle connections after 5min
      - --tcp-keepalive 300                 # TCP keepalive

    ports:
      - "${REDIS_PORT:-6379}:6379"

    volumes:
      - redis_data:/data

    # Health check (critical for API service dependency)
    healthcheck:
      test:
        - CMD
        - redis-cli
        - -a
        - ${REDIS_PASSWORD:-devpassword123}
        - --raw
        - incr
        - ping
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

    # Security hardening
    security_opt:
      - no-new-privileges:true

    # Restart policy
    restart: unless-stopped

    # Resource limits (optional)
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '0.5'
    #       memory: 512M
    #     reservations:
    #       cpus: '0.25'
    #       memory: 256M

    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=redis"

  # ==========================================================================
  # Ollama Service - Local LLM inference (fallback provider)
  # ==========================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: rag_ollama
    hostname: ollama

    # GPU support (ENABLED for faster testing)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    environment:
      # Ollama server configuration
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_ORIGINS=*

      # Performance tuning
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-2}      # Concurrent requests
      - OLLAMA_MAX_LOADED_MODELS=1                          # Only load one model
      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-24h}        # Keep model in memory

      # GPU settings (ENABLED)
      - OLLAMA_LLM_LIBRARY=cublas                          # Use CUDA
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

    ports:
      - "${OLLAMA_PORT:-11434}:11434"

    volumes:
      - ollama_models:/root/.ollama

    # Startup script to pull model
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        set -e
        echo "Starting Ollama server..."
        /bin/ollama serve &
        OLLAMA_PID=$$!

        echo "Waiting for Ollama server to be ready..."
        echo "Timeout: 300 seconds (5 minutes)"
        START_TIME=$$(date +%s)

        for i in $$(seq 1 300); do
          # Use 'ollama list' instead of curl (curl not available in container)
          if ollama list >/dev/null 2>&1; then
            ELAPSED=$$(($$(date +%s) - $$START_TIME))
            echo "Ollama server is ready"
            echo "Time to start: $$ELAPSED seconds"
            break
          fi
          if [ $$i -eq 300 ]; then
            ELAPSED=$$(($$(date +%s) - $$START_TIME))
            echo "ERROR: Ollama server failed to start within 300 seconds"
            echo "Elapsed time: $$ELAPSED seconds"
            exit 1
          fi
          # Progress indicator every 10 seconds
          if [ $$(($$i % 10)) -eq 0 ]; then
            echo "Still waiting... $$i seconds elapsed"
          fi
          sleep 1
        done

        echo "Pulling model: $${LLM_OLLAMA_MODEL:-llama3.1:8b}..."
        MODEL_START=$$(date +%s)
        if ollama pull "$${LLM_OLLAMA_MODEL:-llama3.1:8b}"; then
          MODEL_TIME=$$(($$(date +%s) - $$MODEL_START))
          echo "Model pulled successfully"
          echo "Time to pull model: $$MODEL_TIME seconds"
        else
          echo "WARNING: Failed to pull model, will retry on first request"
        fi

        echo "Ollama ready to serve requests"
        wait $$OLLAMA_PID

    # Health check - monitors Ollama continuously
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s          # Check every 30 seconds
      timeout: 10s           # Fail if takes >10s
      retries: 3             # Mark unhealthy after 3 failures
      start_period: 300s     # 5 minute grace period on startup

    # Restart policy
    restart: unless-stopped

    # Resource limits (adjust based on model size)
    # For llama3.1:8b, recommend at least 8GB RAM
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '4.0'
    #       memory: 12G
    #     reservations:
    #       cpus: '2.0'
    #       memory: 8G

    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=ollama"

  # ==========================================================================
  # Prometheus Service - Metrics collection and alerting
  # ==========================================================================
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: rag_prometheus
    hostname: prometheus

    ports:
      - "9090:9090"

    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro
      - prometheus_data:/prometheus

    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'

    depends_on:
      api:
        condition: service_healthy

    restart: unless-stopped

    # Security hardening
    security_opt:
      - no-new-privileges:true

    # Prometheus runs as nobody (65534) by default
    user: "65534:65534"

    # Health check
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=prometheus,env=development"

  # ==========================================================================
  # Grafana Service - Metrics visualization and dashboards
  # ==========================================================================
  grafana:
    image: grafana/grafana:10.2.2
    container_name: rag_grafana
    hostname: grafana

    ports:
      - "3000:3000"

    environment:
      # Admin credentials (configurable via .env)
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}

      # Server configuration
      - GF_SERVER_ROOT_URL=http://localhost:3000
      - GF_SERVER_SERVE_FROM_SUB_PATH=false

      # Anonymous access disabled
      - GF_AUTH_ANONYMOUS_ENABLED=false

      # Provisioning
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning

      # Default home dashboard
      - GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH=/etc/grafana/dashboards/01-system-overview.json

    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/etc/grafana/dashboards:ro
      - grafana_data:/var/lib/grafana

    depends_on:
      prometheus:
        condition: service_healthy

    restart: unless-stopped

    # Security hardening
    security_opt:
      - no-new-privileges:true

    # Grafana runs as grafana user (472) by default
    user: "472:472"

    # Health check
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=grafana,env=development"

  # ==========================================================================
  # Test Service - Run automated tests
  # ==========================================================================
  # IMPORTANT: This service uses 'test' profile and won't start automatically
  # Usage: docker-compose --profile test up test
  #    or: docker-compose run --rm test
  test:
    profiles: ["test"]  # Only starts when 'test' profile is active
    build:
      context: .
      target: test
    image: personal-rag-system:test
    container_name: rag_test
    hostname: test

    environment:
      - PYTHONUNBUFFERED=1
      - API_URL=http://api:8000
      - OLLAMA_HOST=http://ollama:11434
      - LOG_LEVEL=DEBUG

    env_file:
      - .env

    volumes:
      # Mount test files
      - ./tests:/workspace/tests:ro
      - ./app:/workspace/app:ro
      - ./pytest.ini:/workspace/pytest.ini:ro

      # Mount eval directory for test outputs
      - ./data/eval:/workspace/data/eval

    depends_on:
      api:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_started

    # Security hardening
    security_opt:
      - no-new-privileges:true

    cap_drop:
      - ALL

    tmpfs:
      - /tmp:noexec,nosuid,nodev,size=128m

    user: "65532:65532"

    # Override to run specific tests
    # command: ["/opt/venv/bin/python", "-m", "pytest", "-v", "--tb=short"]

    # Don't restart test service
    restart: "no"

    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "1"
        labels: "service=test"

# ============================================================================
# Named Volumes - Persistent data storage
# ============================================================================
volumes:
  # Redis data persistence (AOF + RDB)
  redis_data:
    driver: local
    name: rag_redis_data

  # Ollama model storage (large files ~5-10GB)
  ollama_models:
    driver: local
    name: rag_ollama_models

  # Prometheus time-series data
  prometheus_data:
    driver: local
    name: rag_prometheus_data

  # Grafana dashboards and settings
  grafana_data:
    driver: local
    name: rag_grafana_data

# ============================================================================
# Networks (optional - Docker Compose creates default network)
# ============================================================================
# Uncomment to use custom network with specific configuration
# networks:
#   rag_network:
#     driver: bridge
#     ipam:
#       config:
#         - subnet: 172.28.0.0/16

# ============================================================================
# Usage Instructions
# ============================================================================
# Start all services:
#   docker-compose up -d
#
# View logs:
#   docker-compose logs -f
#   docker-compose logs -f api
#   docker-compose logs -f redis
#   docker-compose logs -f ollama
#
# Check health:
#   docker-compose ps
#
# Run tests:
#   docker-compose run --rm test
#
# Stop all services:
#   docker-compose down
#
# Remove all data (WARNING: deletes volumes):
#   docker-compose down -v
#
# Rebuild after code changes:
#   docker-compose up --build -d
# ============================================================================
