# ==========================================
# RAG System Configuration Template
# ==========================================
# INSTRUCTIONS:
# 1. Copy this file to .env:  cp .env.example .env
# 2. Replace all values marked with "your-..." or "change-me-..."
# 3. Generate secure keys where indicated
# ==========================================

# ==============================================================================
# Security (CRITICAL - MUST CHANGE!)
# ==============================================================================

# IMPORTANT: Generate a secure random key in production!
# Generate with: openssl rand -hex 32
API_KEY=change-me-to-a-secure-random-key-64-chars

# Redis password for session storage
# Generate with: openssl rand -base64 32
REDIS_PASSWORD=change-me-secure-redis-password

# JWT Secret Key for admin token signing
# Used to sign/verify JWT tokens for admin authentication
# Generate with: openssl rand -hex 32
SECRET_KEY=change-me-to-a-secure-random-key-64-chars

# Admin user credentials (used by scripts/create_admin.py)
# ADMIN_USER=admin
# ADMIN_PASSWORD=your-admin-password


# ==============================================================================
# LLM Provider Settings
# ==============================================================================

# LLM provider: "groq" (fast, free tier) or "deepinfra" (Qwen, better reasoning)
LLM_PROVIDER=groq

# --- Groq Settings ---
# Get your free API key at: https://console.groq.com/keys
LLM_GROQ_API_KEY=your-groq-api-key-here

# Groq model selection:
# OPTION 1: DEVELOPER/TESTING (Default) - Fast, cheap, good enough for testing.
# Cost: ~$0.05/1M tokens | Rate Limit: High
LLM_GROQ_MODEL=llama-3.1-8b-instant

# OPTION 2: PRODUCTION/REASONING - Smarter, 12x cost, better for complex RAG.
# Use this for actual deployment or when testing reasoning quality.
# Cost: ~$0.59/1M tokens | Rate Limit: 1000 requests/min (Developer Tier)
# LLM_GROQ_MODEL=llama-3.3-70b-versatile

# --- DeepInfra Settings ---
# Get your API key at: https://deepinfra.com/dash/api_keys
# Required if LLM_PROVIDER=deepinfra
LLM_DEEPINFRA_API_KEY=your-deepinfra-api-key-here

# DeepInfra model selection:
# Qwen 3 32B: Best reasoning, $0.08/1M input tokens
LLM_DEEPINFRA_MODEL=Qwen/Qwen3-32B

# --- Shared LLM Settings ---
# Temperature: Lower = more deterministic (0.0-2.0)
LLM_TEMPERATURE=0.1

# Maximum tokens to generate
LLM_MAX_TOKENS=1000

# Context window size (tokens)
LLM_NUM_CTX=4096


# ==============================================================================
# Embedding Model
# ==============================================================================

# SentenceTransformer model for embeddings
# BGE-small-en-v1.5 is optimized for retrieval tasks
EMBED_MODEL=BAAI/bge-small-en-v1.5


# ==============================================================================
# Data Paths
# ==============================================================================

# Directory containing your markdown documents
# (resume, transcripts, certifications)
DOCS_DIR=/workspace/data/mds

# ChromaDB vector database storage
CHROMA_DIR=./data/chroma

# ChromaDB collection name
COLLECTION_NAME=personal_rag


# ==============================================================================
# Ingestion Configuration
# ==============================================================================

# Comma-separated list of file extensions accepted during ingestion
INGEST_ALLOWED_EXTENSIONS=txt,md

# Maximum file size (bytes) allowed for ingestion (default 10 MB)
INGEST_MAX_FILE_SIZE=10485760

# Number of documents processed per ingestion batch
INGEST_BATCH_SIZE=100


# ==============================================================================
# Retrieval Settings
# ==============================================================================

# Target chunk size in characters for retrieval defaults
# Increased to 800 to accommodate prose-formatted transcript terms (longest: ~700 chars)
CHUNK_SIZE=800

# Overlap between chunks (helps with context continuity)
CHUNK_OVERLAP=120

# Number of chunks to send to LLM after reranking
# Keep this small (5-10) for focused context
TOP_K=5

# Maximum cosine distance for retrieval
# 0 = identical, 2 = opposite
# Lower = more strict, higher = more lenient
# Good range: 0.50-0.70
MAX_DISTANCE=0.60

# Distance threshold for grounding check
# If best chunk distance > this, answer "I don't know"
# Should be ≤ MAX_DISTANCE
# Start at 0.60, tune based on evaluation
NULL_THRESHOLD=0.60

# Enable hybrid reranking (combines lexical + semantic matching)
# This helps exact term matches (like "CS 410" or "AWS") rank higher
RERANK=true

# Weight for lexical vs semantic similarity in reranking (0.0-1.0)
# 0.0 = pure semantic, 1.0 = pure lexical, 0.5 = balanced
# Higher values favor exact keyword matches
# Reduced from 0.6 to 0.3 to prioritize semantic similarity for negative inference
RERANK_LEX_WEIGHT=0.3

# Number of chunks to retrieve when reranking is enabled (before reranking)
# Set to collection size (50) to ensure we never miss relevant chunks
# With only 50 total chunks, retrieving all is fast and guarantees best results
# After reranking, only the top TOP_K chunks are sent to the LLM
RERANK_RETRIEVAL_K=50


# ==============================================================================
# Query Router Control
# ==============================================================================

# Enable intelligent query routing for metadata filtering and optimization
# Set to false to use simple retrieve→rerank without domain-based filtering
# Set to true to enable router's domain detection and metadata filtering
# For small collections (<100 chunks), false is recommended to avoid over-filtering
USE_ROUTER=false


# ==============================================================================
# Negative Inference Settings
# ==============================================================================

# Threshold method for detecting if entities exist in knowledge base
# Options: 'fixed', 'gap_based', 'context_aware'
# - fixed: Use NEGATIVE_INFERENCE_THRESHOLD (simple, fast)
# - gap_based: Analyze distance gaps between results (more accurate, recommended)
# - context_aware: Entity-type specific thresholds (experimental)
NEGATIVE_INFERENCE_METHOD=gap_based

# Fixed threshold for entity existence (only used if METHOD=fixed)
# Entities with distance > this are considered "not found"
# Range: 0.0-1.0, recommended: 0.35-0.40
NEGATIVE_INFERENCE_THRESHOLD=0.37


# ==============================================================================
# Query Rewriting System
# ==============================================================================

# Enable/disable pattern-based query rewriting
# ✅ ENABLED: Only negative_inference pattern is enabled (helps with "Do I have X?" queries)
# All other patterns disabled after evaluation showed regressions
# See QUERY_EXPANSION_ANALYSIS_AND_RECOMMENDATIONS.md for details
QUERY_REWRITER_ENABLED=true

# Path to pattern configuration YAML
QUERY_REWRITER_CONFIG=config/query_patterns.yaml

# Enable hot-reloading of pattern config
# Patterns will be reloaded every RELOAD_INTERVAL seconds without restart
QUERY_REWRITER_HOT_RELOAD=true

# Hot-reload check interval (seconds)
QUERY_REWRITER_RELOAD_INTERVAL=60

# Enable pattern analytics tracking
# Tracks match rates, success rates, latency, and distance improvements
QUERY_REWRITER_ANALYTICS=true

# Path to analytics JSON file
QUERY_REWRITER_ANALYTICS_PATH=data/analytics/pattern_effectiveness.json

# Path to failed queries JSON file
QUERY_REWRITER_FAILED_PATH=data/analytics/failed_queries.json

# Maximum allowed latency for query rewriting (milliseconds)
# Queries exceeding this will log a warning
QUERY_REWRITER_MAX_LATENCY=10.0

# Distance threshold for capturing failed queries
# Queries with best distance > this are captured for pattern suggestion
QUERY_REWRITER_FAILED_THRESHOLD=0.5


# ==============================================================================
# Cross-Encoder Reranking (Phase 2)
# ==============================================================================

# Enable/disable cross-encoder neural reranking
# When enabled, uses two-stage reranking: Hybrid (fast) → Cross-encoder (precise)
# Disabled by default - enable after testing to ensure acceptable latency
CROSS_ENCODER_ENABLED=true

# HuggingFace cross-encoder model
# Options:
# - cross-encoder/ms-marco-MiniLM-L-6-v2: Faster, ~150ms for 10 pairs (recommended)
# - cross-encoder/ms-marco-MiniLM-L-12-v2: More accurate, ~250ms for 10 pairs
CROSS_ENCODER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# Directory to cache model files
CROSS_ENCODER_CACHE_DIR=/tmp/cross-encoder

# Number of chunks to retrieve before any reranking
# Stage 1 (Hybrid) will filter these down to INTERMEDIATE_K
CROSS_ENCODER_RETRIEVAL_K=15

# Final number of chunks after cross-encoder reranking
# These chunks are sent to the LLM
CROSS_ENCODER_TOP_K=5

# Maximum acceptable latency for cross-encoder (milliseconds)
# Warning logged if exceeded, but reranking still proceeds
# Target: 150ms for L-6, up to 300ms for L-12
CROSS_ENCODER_MAX_LATENCY_MS=500.0


# ==============================================================================
# HTTP Settings
# ==============================================================================

# Allowed Origins (CORS) - comma-separated list
# IMPORTANT: Update for production with your actual domain!
ALLOWED_ORIGINS=https://your-domain.com,http://localhost:3000

# Maximum request body size (bytes)
MAX_BYTES=32768


# ==============================================================================
# Session Management Configuration
# ==============================================================================

# Storage Backend
# Options: "redis" (production) or "memory" (development/fallback)
SESSION_STORAGE_BACKEND=redis

# Redis Connection
# For Docker: redis://:PASSWORD@redis:6379/0
# For local development: redis://:PASSWORD@localhost:6379/0
# Note: ${REDIS_PASSWORD} will be substituted from the variable above
SESSION_REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379/0

# Session Limits
# Maximum total active sessions
SESSION_MAX_TOTAL_SESSIONS=1000

# Maximum sessions per IP address
SESSION_MAX_PER_IP=5

# Rate Limiting
# Maximum queries per session per hour (0 = disabled)
SESSION_QUERIES_PER_HOUR=10

# TTL and Cleanup
# Session time-to-live (6 hours = 21600 seconds)
SESSION_TTL_SECONDS=21600

# Conversation History
# Maximum tokens for conversation history
SESSION_MAX_HISTORY_TOKENS=250

# Maximum conversation turns to keep
SESSION_MAX_HISTORY_TURNS=5

# Security
# Set to true in production when using HTTPS
SESSION_REQUIRE_HTTPS=false


# ==============================================================================
# Response Caching Configuration
# ==============================================================================

# Enable/disable response caching for common queries
# Caching stores complete responses in Redis for faster retrieval
# Target: <100ms for cache hits vs 1-3s for full RAG pipeline
RESPONSE_CACHE_ENABLED=true

# Time-to-live for cached responses (seconds)
# Default: 3600 (1 hour)
# Increase for more stable data, decrease for frequently changing data
RESPONSE_CACHE_TTL_SECONDS=3600

# Maximum cache size (MB) - soft limit
# Redis will use LRU eviction when maxmemory is reached
RESPONSE_CACHE_MAX_SIZE_MB=100

# Prompt version for cache invalidation
# Increment this when you change the system prompt to invalidate old cached responses
RESPONSE_CACHE_PROMPT_VERSION=1


# ==============================================================================
# BM25 Settings (Keyword Search)
# ==============================================================================

# Term frequency saturation parameter (typical: 1.2-2.0)
BM25_K1=1.5

# Document length normalization parameter (typical: 0-1)
BM25_B=0.5

# Reciprocal Rank Fusion parameter for combining rankings
BM25_RRF_K=60


# ==============================================================================
# Prompt Guard Settings (Security)
# ==============================================================================

# Enable/disable prompt injection guard
PROMPT_GUARD_ENABLED=true

# Groq model for prompt guard (86m for accuracy, 22m for speed)
PROMPT_GUARD_MODEL=meta-llama/llama-prompt-guard-2-86m

# If True, allow requests when guard errors; if False, block on error
PROMPT_GUARD_FAIL_OPEN=true

# Timeout for Groq API calls in seconds
PROMPT_GUARD_TIMEOUT_SECONDS=3.0

# Maximum number of retry attempts on transient failures
PROMPT_GUARD_MAX_RETRIES=2

# Cache TTL for prompt guard results in seconds
PROMPT_GUARD_CACHE_TTL_SECONDS=3600

# Maximum number of entries in LRU cache
PROMPT_GUARD_CACHE_MAX_SIZE=1000


# ==============================================================================
# Monitoring Configuration (Prometheus + Grafana)
# ==============================================================================

# Grafana Admin Credentials
# IMPORTANT: Change these in production!
GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=change-me-secure-grafana-password

# Prometheus Data Retention (days)
PROMETHEUS_RETENTION_DAYS=7

# Grafana Configuration
GRAFANA_ALLOW_SIGNUP=false
GRAFANA_ANALYTICS_REPORTING=false


# ==============================================================================
# Alertmanager SMTP Configuration
# ==============================================================================
SMTP_FROM=alerts@yourdomain.com
SMTP_USERNAME=your-smtp-username
SMTP_PASSWORD=your-smtp-password
ALERT_EMAIL=admin@yourdomain.com


# ==============================================================================
# Database (PostgreSQL)
# ==============================================================================
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_USER=rag_user
POSTGRES_PASSWORD=change-me-secure-postgres-password
POSTGRES_DB=rag_db


# ==============================================================================
# Environment Type (Optional)
# ==============================================================================

# Set to "production" to enable production-specific validation warnings
# ENV=production

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO
