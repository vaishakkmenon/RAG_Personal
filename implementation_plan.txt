Implementation Plan: Top 5 Production Readiness Items

 Overview

 This plan covers the implementation of the 5 critical production readiness items for the Personal RAG System, prioritized by impact and dependency order.

 ---
 Priority Order

 1. Alertmanager Setup (Immediate visibility into issues)
 2. Container Registry (GHCR) (Enable proper CI/CD flow)
 3. Staging Environment (Safe testing before production)
 4. Backup Restoration Testing (Verify disaster recovery)
 5. Data Encryption at Rest (Infrastructure hardening)

 ---
 1. Alertmanager Setup (Email Notifications)

 Goal

 Configure Prometheus Alertmanager to send email alerts for critical issues.

 Files to Create/Modify

 | File                                      | Action                                    |
 |-------------------------------------------|-------------------------------------------|
 | monitoring/prometheus/alertmanager.yml    | CREATE - Alertmanager configuration       |
 | monitoring/prometheus/prometheus.yml      | MODIFY - Enable Alertmanager target       |
 | monitoring/prometheus/prometheus.prod.yml | MODIFY - Enable Alertmanager target       |
 | monitoring/prometheus/alerts.yml          | MODIFY - Add more alert rules             |
 | docker-compose.yml                        | MODIFY - Add Alertmanager service         |
 | docker-compose.prod.yml                   | MODIFY - Add Alertmanager service         |
 | .env.example                              | MODIFY - Add SMTP configuration variables |

 Implementation Steps

 Step 1: Create Alertmanager configuration
 # monitoring/prometheus/alertmanager.yml
 global:
   smtp_smarthost: 'smtp.gmail.com:587'
   smtp_from: '${SMTP_FROM}'
   smtp_auth_username: '${SMTP_USERNAME}'
   smtp_auth_password: '${SMTP_PASSWORD}'
   smtp_require_tls: true

 route:
   group_by: ['alertname', 'severity']
   group_wait: 30s
   group_interval: 5m
   repeat_interval: 4h
   receiver: 'email-notifications'
   routes:
     - match:
         severity: critical
       receiver: 'email-notifications'
       repeat_interval: 1h

 receivers:
   - name: 'email-notifications'
     email_configs:
       - to: '${ALERT_EMAIL}'
         send_resolved: true

 inhibit_rules:
   - source_match:
       severity: 'critical'
     target_match:
       severity: 'warning'
     equal: ['alertname']

 Step 2: Add Alertmanager service to docker-compose.yml (after prometheus service ~line 428)
 alertmanager:
   image: prom/alertmanager:v0.26.0
   container_name: rag_alertmanager
   hostname: alertmanager
   ports:
     - "127.0.0.1:9093:9093"
   volumes:
     - ./monitoring/prometheus/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
     - alertmanager_data:/alertmanager
   command:
     - '--config.file=/etc/alertmanager/alertmanager.yml'
     - '--storage.path=/alertmanager'
   environment:
     - SMTP_FROM=${SMTP_FROM}
     - SMTP_USERNAME=${SMTP_USERNAME}
     - SMTP_PASSWORD=${SMTP_PASSWORD}
     - ALERT_EMAIL=${ALERT_EMAIL}
   restart: unless-stopped
   security_opt:
     - no-new-privileges:true
   networks:
     - rag_network

 Step 3: Update Prometheus config to point to Alertmanager
 # In prometheus.yml and prometheus.prod.yml, update alerting section:
 alerting:
   alertmanagers:
     - static_configs:
         - targets: ['alertmanager:9093']

 Step 4: Add more alert rules to alerts.yml
 - RedisDown: Redis connection failures
 - PostgresDown: PostgreSQL unreachable
 - CacheHitRateLow: Cache hit rate below 50%
 - LLMErrorsHigh: LLM API errors above threshold
 - DiskSpaceLow: Disk usage above 85%

 Step 5: Add SMTP variables to .env.example
 # Alertmanager SMTP Configuration
 SMTP_FROM=alerts@yourdomain.com
 SMTP_USERNAME=your-smtp-username
 SMTP_PASSWORD=your-smtp-password
 ALERT_EMAIL=admin@yourdomain.com

 Verification

 - Access Alertmanager UI at http://localhost:9093
 - Trigger a test alert and verify email delivery
 - Check alert status in Prometheus UI at http://localhost:9090/alerts

 ---
 2. Container Registry (GHCR)

 Goal

 Push Docker images to GitHub Container Registry and automate image tagging with git commit SHA.

 Files to Create/Modify

 | File                         | Action                            |
 |------------------------------|-----------------------------------|
 | .github/workflows/ci.yml     | MODIFY - Add build and push steps |
 | .github/workflows/deploy.yml | CREATE - Optional CD workflow     |

 Implementation Steps

 Step 1: Update CI workflow to build and push to GHCR

 Add to .github/workflows/ci.yml after the linter step:

   build-and-push:
     runs-on: ubuntu-latest
     needs: docker-ci  # Only push if tests pass
     if: github.ref == 'refs/heads/main'  # Only on main branch
     permissions:
       contents: read
       packages: write
     steps:
       - name: Checkout code
         uses: actions/checkout@v4

       - name: Set up Docker Buildx
         uses: docker/setup-buildx-action@v3

       - name: Log in to GHCR
         uses: docker/login-action@v3
         with:
           registry: ghcr.io
           username: ${{ github.actor }}
           password: ${{ secrets.GITHUB_TOKEN }}

       - name: Extract metadata
         id: meta
         uses: docker/metadata-action@v5
         with:
           images: ghcr.io/${{ github.repository }}
           tags: |
             type=sha,prefix=
             type=ref,event=branch
             type=raw,value=latest,enable=${{ github.ref == 'refs/heads/main' }}

       - name: Build and push
         uses: docker/build-push-action@v5
         with:
           context: .
           file: ./Dockerfile.prod
           target: runtime
           push: true
           tags: ${{ steps.meta.outputs.tags }}
           labels: ${{ steps.meta.outputs.labels }}
           cache-from: type=gha
           cache-to: type=gha,mode=max

 Step 2: Update docker-compose.prod.yml to use GHCR image

 # In docker-compose.prod.yml, update api service:
 api:
   image: ghcr.io/YOUR_GITHUB_USERNAME/rag_personal:${IMAGE_TAG:-latest}
   # Remove build section for production (use pre-built image)

 Step 3: Update deploy.sh to pull from GHCR

 #!/bin/bash
 # Pull latest image from GHCR
 docker compose -f docker-compose.prod.yml pull api
 docker compose -f docker-compose.prod.yml up -d

 Verification

 - Push to main branch and verify image appears in GitHub Packages
 - Check image tags: ghcr.io/username/rag_personal:latest, ghcr.io/username/rag_personal:sha-abc123
 - Pull image on VPS and verify it runs correctly

 ---
 3. Staging Environment

 Goal

 Create isolated staging environment on same server using Docker network isolation.

 Files to Create/Modify

 | File                       | Action                                |
 |----------------------------|---------------------------------------|
 | docker-compose.staging.yml | CREATE - Staging compose file         |
 | .env.staging.example       | CREATE - Staging environment template |
 | scripts/deploy-staging.sh  | CREATE - Staging deployment script    |

 Implementation Steps

 Step 1: Create docker-compose.staging.yml

 Based on docker-compose.prod.yml with these differences:
 - Separate network: rag_network_staging
 - Separate volumes: *_staging suffix
 - Different container names: *_staging suffix
 - Ports bound to localhost with offset (internal access only)
 - Separate .env.staging file

 name: personal-rag-system-staging

 services:
   api:
     image: ghcr.io/YOUR_USERNAME/rag_personal:${IMAGE_TAG:-latest}
     container_name: rag_api_staging
     # No external ports - access via Caddy only
     environment:
       - LOG_LEVEL=DEBUG  # More verbose for staging
       # ... (copy from prod with staging-specific values)
     env_file:
       - .env.staging
     volumes:
       - ./data/chroma_staging:/workspace/data/chroma
       - ./data/docs:/workspace/data/docs:ro
       - ./data/mds:/workspace/data/mds:ro
     networks:
       - rag_network_staging
     # ... (same security settings as prod)

   redis:
     container_name: rag_redis_staging
     volumes:
       - redis_data_staging:/data
     networks:
       - rag_network_staging

   # Caddy for staging (optional - or use port forwarding)
   caddy:
     container_name: rag_caddy_staging
     command: caddy reverse-proxy --from staging.api.vaishakmenon.com --to api:8000
     networks:
       - rag_network_staging

 networks:
   rag_network_staging:
     driver: bridge
     name: rag_network_staging
     internal: false

 volumes:
   redis_data_staging:
   postgres_data_staging:
   # ... staging-specific volumes

 Step 2: Create .env.staging.example

 Copy from .env.example with staging-specific defaults:
 - SESSION_QUERIES_PER_HOUR=100 (more lenient for testing)
 - LOG_LEVEL=DEBUG
 - Separate database credentials

 Step 3: Create scripts/deploy-staging.sh

 #!/bin/bash
 set -e
 echo "Deploying to staging..."
 docker compose -f docker-compose.staging.yml pull
 docker compose -f docker-compose.staging.yml up -d
 echo "Staging deployed. Access at https://staging.api.vaishakmenon.com"

 Verification

 - Run staging alongside production without conflicts
 - Verify network isolation (staging can't reach prod databases)
 - Test deployment flow: dev → staging → prod

 ---
 4. Backup Restoration Testing

 Goal

 Create restoration script and document disaster recovery procedure.

 Files to Create/Modify

 | File                      | Action                         |
 |---------------------------|--------------------------------|
 | scripts/restore.sh        | CREATE - Restoration script    |
 | docs/disaster_recovery.md | CREATE - DR documentation      |
 | scripts/backup.sh         | MODIFY - Add verification step |

 Implementation Steps

 Step 1: Create scripts/restore.sh

 #!/bin/bash
 set -e

 BACKUP_DIR="${BACKUP_DIR:-/backups}"
 DATE="${1:-$(ls -t $BACKUP_DIR/db_*.sql.gz | head -1 | grep -oP '\d{4}-\d{2}-\d{2}')}"

 echo "Restoring from backup dated: $DATE"
 echo "WARNING: This will overwrite current data. Press Ctrl+C to cancel."
 sleep 5

 # Stop services (except databases)
 docker compose stop api caddy

 # Restore PostgreSQL
 echo "Restoring PostgreSQL..."
 if [ -f "$BACKUP_DIR/db_${DATE}.sql.gz" ]; then
     gunzip -c "$BACKUP_DIR/db_${DATE}.sql.gz" | \
         docker compose exec -T postgres psql -U postgres -d rag_feedback
     echo "PostgreSQL restored."
 else
     echo "WARNING: PostgreSQL backup not found: db_${DATE}.sql.gz"
 fi

 # Restore Redis
 echo "Restoring Redis..."
 docker compose stop redis
 if [ -f "$BACKUP_DIR/redis_${DATE}.rdb" ]; then
     cp "$BACKUP_DIR/redis_${DATE}.rdb" ./data/redis/dump.rdb
     echo "Redis RDB copied."
 else
     echo "WARNING: Redis backup not found: redis_${DATE}.rdb"
 fi
 docker compose start redis

 # Restore ChromaDB
 echo "Restoring ChromaDB..."
 if [ -f "$BACKUP_DIR/chroma_${DATE}.tar.gz" ]; then
     rm -rf ./data/chroma/*
     tar -xzf "$BACKUP_DIR/chroma_${DATE}.tar.gz" -C ./data/chroma
     echo "ChromaDB restored."
 else
     echo "WARNING: ChromaDB backup not found: chroma_${DATE}.tar.gz"
 fi

 # Restart services
 docker compose up -d

 echo "Restoration complete. Verify with: docker compose ps"
 echo "Run health check: curl http://localhost:8000/health/detailed"

 Step 2: Add verification to backup.sh

 Add after each backup operation:
 # Verify backup integrity
 echo "Verifying backups..."
 gzip -t "$BACKUP_DIR/db_${DATE}.sql.gz" && echo "PostgreSQL backup OK"
 tar -tzf "$BACKUP_DIR/chroma_${DATE}.tar.gz" > /dev/null && echo "ChromaDB backup OK"

 Step 3: Create docs/disaster_recovery.md

 Document:
 - Backup schedule and retention
 - RTO/RPO targets
 - Step-by-step restoration procedure
 - Verification checklist
 - Contact information

 Verification

 - Run backup, delete data, run restore, verify data integrity
 - Time the restoration process to establish RTO
 - Document actual vs expected recovery time

 ---
 5. Data Encryption at Rest (Infrastructure-Level, Free)

 Goal

 Enable encryption for Redis, PostgreSQL, and backups using free tools.

 Files to Create/Modify

 | File                       | Action                                   |
 |----------------------------|------------------------------------------|
 | docker-compose.yml         | MODIFY - Add encryption configs          |
 | docker-compose.prod.yml    | MODIFY - Add encryption configs          |
 | scripts/backup.sh          | MODIFY - Encrypt backups with GPG        |
 | app/services/encryption.py | CREATE - Optional data encryption helper |

 Implementation Steps

 Step 1: Enable PostgreSQL column-level encryption with pgcrypto

 Add to docker-compose.yml postgres service:
 postgres:
   # ... existing config
   command:
     - -c
     - shared_preload_libraries=pgcrypto

 Create migration to encrypt sensitive columns:
 -- Enable pgcrypto extension
 CREATE EXTENSION IF NOT EXISTS pgcrypto;

 -- Example: Encrypt session data (if storing sensitive info)
 -- Use pgp_sym_encrypt/pgp_sym_decrypt with app-level key

 Step 2: Enable Redis TLS (free, built into Redis 6+)

 Note: This requires Redis to be compiled with TLS support. The redis:7-alpine image has TLS support.

 Update docker-compose.yml redis service:
 redis:
   command:
     - redis-server
     - --appendonly yes
     - --requirepass ${REDIS_PASSWORD}
     - --tls-port 6379
     - --port 0
     - --tls-cert-file /tls/redis.crt
     - --tls-key-file /tls/redis.key
     - --tls-ca-cert-file /tls/ca.crt
   volumes:
     - ./secrets/redis-tls:/tls:ro

 Generate self-signed certificates:
 # scripts/generate-redis-tls.sh
 mkdir -p secrets/redis-tls
 openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
   -keyout secrets/redis-tls/redis.key \
   -out secrets/redis-tls/redis.crt \
   -subj "/CN=redis"
 cp secrets/redis-tls/redis.crt secrets/redis-tls/ca.crt

 Update Redis connection URL:
 # In app/settings.py
 SESSION_REDIS_URL = "rediss://:password@redis:6379/0?ssl_cert_reqs=required"

 Step 3: Encrypt backups with GPG

 Update scripts/backup.sh:
 # After creating each backup, encrypt it
 gpg --symmetric --cipher-algo AES256 --batch --passphrase-file /run/secrets/backup_key \
     -o "$BACKUP_DIR/db_${DATE}.sql.gz.gpg" "$BACKUP_DIR/db_${DATE}.sql.gz"
 rm "$BACKUP_DIR/db_${DATE}.sql.gz"  # Remove unencrypted version

 Update scripts/restore.sh:
 # Decrypt before restoring
 gpg --decrypt --batch --passphrase-file /run/secrets/backup_key \
     "$BACKUP_DIR/db_${DATE}.sql.gz.gpg" > "$BACKUP_DIR/db_${DATE}.sql.gz"

 Verification

 - Verify PostgreSQL pgcrypto: docker exec rag_postgres psql -U postgres -c "SELECT pgp_sym_encrypt('test', 'key');"
 - Verify Redis TLS: redis-cli --tls --cacert ca.crt ping
 - Verify backup encryption: file backup.sql.gz.gpg should show "GPG encrypted data"

 ---
 Implementation Order & Dependencies

 Week 1:
 ├── Day 1-2: Alertmanager Setup
 │   └── Immediate visibility into production issues
 ├── Day 3-4: Container Registry (GHCR)
 │   └── Enables proper image management
 └── Day 5: Staging Environment
     └── Uses GHCR images for deployment testing

 Week 2:
 ├── Day 1-2: Backup Restoration Testing
 │   └── Can test on staging environment first
 └── Day 3-5: Data Encryption
     └── Most complex, test thoroughly on staging

 ---
 Summary

 | Item                | Effort | Impact   | Risk   |
 |---------------------|--------|----------|--------|
 | Alertmanager        | Low    | High     | Low    |
 | Container Registry  | Medium | High     | Low    |
 | Staging Environment | Medium | High     | Low    |
 | Backup Restoration  | Low    | Critical | Low    |
 | Data Encryption     | High   | High     | Medium |

 Total estimated effort: 1-2 weeks for full implementation
